Extending Double Q-learning to DQN is Double-DQN
 
 Dualing DQN, when there is no big difference in action values,
low level and high level control?

Advantage function: A(s,a) = V(s) - Q(s,a)
Q(s,a) = V(s) - A(s,a)

learn the v function and A, instead of the Q function
Use that Q function for updation.
- pick actions according to the Q, then mesaure the performance
- backprop and learn V and A.


Implementation_details
- should we use another fc layer after common features?[done, it should be i think]
- Implement the DQN tutorial agent code.
- so that means i do not need the traget network right?
    - we need it or else how do we even compute it.
- and graidient clipping a hueristic to prevent the problem of exploding gradients
    - value clipping: if graident is greater than a value then replace it with this value
    - norm clipping: scale the whole gradient for a norm value if norm exceeds a limit this way the directio
        of the graident vector is preserved.
- verify how the gradients work when you have the architecture you defined, just 
    read the code from top and understand all the aspects of it.
