{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "05tpa2v9Axle"
      },
      "source": [
        "# MC REINFORCE -TYPE -I\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HnU1lc-hA35F"
      },
      "source": [
        "# IMPORTS\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_brAq5tlA5Gj"
      },
      "outputs": [],
      "source": [
        "# '''\n",
        "# Installing packages for rendering the game on Colab\n",
        "# '''\n",
        "\n",
        "# !pip install gym pyvirtualdisplay > /dev/null 2>&1\n",
        "# !apt-get install -y xvfb python-opengl ffmpeg > /dev/null 2>&1\n",
        "# !apt-get update > /dev/null 2>&1\n",
        "# !apt-get install cmake > /dev/null 2>&1\n",
        "# !pip install --upgrade setuptools 2>&1\n",
        "# !pip install ez_setup > /dev/null 2>&1\n",
        "# !pip install gym[atari] > /dev/null 2>&1\n",
        "# !pip install git+https://github.com/tensorflow/docs > /dev/null 2>&1\n",
        "# !pip install gym[classic_control]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aLQe6Spe0qCs",
        "outputId": "7f80ddb3-93b6-4705-c1ec-ed301401b099"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting pyvirtualdisplay\n",
            "  Downloading PyVirtualDisplay-3.0-py3-none-any.whl (15 kB)\n",
            "Installing collected packages: pyvirtualdisplay\n",
            "Successfully installed pyvirtualdisplay-3.0\n"
          ]
        }
      ],
      "source": [
        "!pip install pyvirtualdisplay"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q3oPUaQ9BGDf",
        "outputId": "efd898bd-67ae-4a0b-91dc-7d619e9957cf"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/tensorflow_probability/python/__init__.py:57: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
            "  if (distutils.version.LooseVersion(tf.__version__) <\n"
          ]
        }
      ],
      "source": [
        "'''\n",
        "A bunch of imports, you don't have to worry about these\n",
        "'''\n",
        "\n",
        "import numpy as np\n",
        "import random\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from collections import namedtuple, deque\n",
        "import torch.optim as optim\n",
        "import datetime\n",
        "import gym\n",
        "from gym.wrappers.record_video import RecordVideo\n",
        "import glob\n",
        "import io\n",
        "import base64\n",
        "import matplotlib.pyplot as plt\n",
        "from IPython.display import HTML\n",
        "from pyvirtualdisplay import Display\n",
        "import tensorflow as tf\n",
        "from IPython import display as ipythondisplay\n",
        "from PIL import Image\n",
        "import tensorflow_probability as tfp"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a10gcO7LBh8-"
      },
      "outputs": [],
      "source": [
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "55TEjWLXBkIR",
        "outputId": "37db1a90-3251-4226-a0bc-a5a108cd78d2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "4\n",
            "2\n",
            "0\n",
            "----\n",
            "[ 0.01369617 -0.02302133 -0.04590265 -0.04834723]\n",
            "----\n",
            "1\n",
            "----\n",
            "[ 0.01323574  0.17272775 -0.04686959 -0.3551522 ]\n",
            "1.0\n",
            "False\n",
            "{}\n",
            "----\n"
          ]
        }
      ],
      "source": [
        "'''\n",
        "Please refer to the first tutorial for more details on the specifics of environments\n",
        "We've only added important commands you might find useful for experiments.\n",
        "'''\n",
        "\n",
        "'''\n",
        "List of example environments\n",
        "(Source - https://gym.openai.com/envs/#classic_control)\n",
        "\n",
        "'Acrobot-v1'\n",
        "'Cartpole-v1'\n",
        "'MountainCar-v0'\n",
        "'''\n",
        "\n",
        "env = gym.make('CartPole-v1')\n",
        "env.seed(0)\n",
        "\n",
        "state_shape = env.observation_space.shape[0]\n",
        "no_of_actions = env.action_space.n\n",
        "\n",
        "print(state_shape)\n",
        "print(no_of_actions)\n",
        "print(env.action_space.sample())\n",
        "print(\"----\")\n",
        "\n",
        "'''\n",
        "# Understanding State, Action, Reward Dynamics\n",
        "\n",
        "The agent decides an action to take depending on the state.\n",
        "\n",
        "The Environment keeps a variable specifically for the current state.\n",
        "- Everytime an action is passed to the environment, it calculates the new state and updates the current state variable.\n",
        "- It returns the new current state and reward for the agent to take the next action\n",
        "\n",
        "'''\n",
        "\n",
        "state = env.reset()\n",
        "''' This returns the initial state (when environment is reset) '''\n",
        "\n",
        "print(state)\n",
        "print(\"----\")\n",
        "\n",
        "action = env.action_space.sample()\n",
        "''' We take a random action now '''\n",
        "\n",
        "print(action)\n",
        "print(\"----\")\n",
        "\n",
        "next_state, reward, done, info = env.step(action)\n",
        "''' env.step is used to calculate new state and obtain reward based on old state and action taken  '''\n",
        "\n",
        "print(next_state)\n",
        "print(reward)\n",
        "print(done)\n",
        "print(info)\n",
        "print(\"----\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j0NCn20-EAK-"
      },
      "source": [
        "# NETWORK FOR J(Î¸)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hdFsxJgWCbA1"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "### Q Network & Some 'hyperparameters'\n",
        "\n",
        "QNetwork1:\n",
        "Input Layer - 4 nodes (State Shape) \\\n",
        "Hidden Layer 1 - 128 nodes \\\n",
        "Hidden Layer 2 - 64 nodes \\\n",
        "Output Layer - 2 nodes (Action Space) \\\n",
        "Optimizer - zero_grad()\n",
        "'''\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "\n",
        "'''\n",
        "Bunch of Hyper parameters (Which you might have to tune later)\n",
        "'''\n",
        "BUFFER_SIZE = int(1e5)  # replay buffer size\n",
        "BATCH_SIZE = 64         # minibatch size\n",
        "GAMMA = 0.99            # discount factor\n",
        "LR = 5e-4               # learning rate\n",
        "UPDATE_EVERY = 20       # how often to update the network (When Q target is present)\n",
        "\n",
        "\n",
        "class QNetwork1(nn.Module):\n",
        "\n",
        "    def __init__(self, state_size, action_size, seed, fc1_units=128, fc2_units=64):\n",
        "        \"\"\"Initialize parameters and build model.\n",
        "        Params\n",
        "        ======\n",
        "            state_size (int): Dimension of each state\n",
        "            action_size (int): Dimension of each action\n",
        "            seed (int): Random seed\n",
        "            fc1_units (int): Number of nodes in first hidden layer\n",
        "            fc2_units (int): Number of nodes in second hidden layer\n",
        "        \"\"\"\n",
        "        super(QNetwork1, self).__init__()\n",
        "        self.seed = torch.manual_seed(seed)\n",
        "        self.fc1 = nn.Linear(state_size, fc1_units)\n",
        "        self.fc2 = nn.Linear(fc1_units, fc2_units)\n",
        "        self.fc3 = nn.Linear(fc2_units, action_size)\n",
        "\n",
        "    def forward(self, state):\n",
        "        \"\"\"Build a network that maps state -> action values.\"\"\"\n",
        "        x = F.relu(self.fc1(state))\n",
        "        x = F.relu(self.fc2(x))\n",
        "        return self.fc3(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gEM3hG3CFCOz"
      },
      "outputs": [],
      "source": [
        "class TutorialAgent():\n",
        "\n",
        "    def __init__(self, state_size, action_size, seed):\n",
        "\n",
        "        ''' Agent Environment Interaction '''\n",
        "        self.state_size = state_size\n",
        "        self.action_size = action_size\n",
        "        self.seed = random.seed(seed)\n",
        "\n",
        "        ''' Q-Network '''\n",
        "        self.qnetwork_local = QNetwork1(state_size, action_size, seed).to(device)\n",
        "        self.qnetwork_target = QNetwork1(state_size, action_size, seed).to(device)\n",
        "        self.optimizer = optim.Adam(self.qnetwork_local.parameters(), lr=LR)\n",
        "\n",
        "        ''' Replay memory '''\n",
        "        self.memory = ReplayBuffer(action_size, BUFFER_SIZE, BATCH_SIZE, seed)\n",
        "\n",
        "        ''' Initialize time step (for updating every UPDATE_EVERY steps)           -Needed for Q Targets '''\n",
        "        self.t_step = 0\n",
        "\n",
        "    def step(self, state, action, reward, next_state, done):\n",
        "\n",
        "        ''' Save experience in replay memory '''\n",
        "        self.memory.add(state, action, reward, next_state, done)\n",
        "\n",
        "        ''' If enough samples are available in memory, get random subset and learn '''\n",
        "        if len(self.memory) >= BATCH_SIZE:\n",
        "            experiences = self.memory.sample()\n",
        "            self.learn(experiences, GAMMA)\n",
        "\n",
        "        \"\"\" +Q TARGETS PRESENT \"\"\"\n",
        "        ''' Updating the Network every 'UPDATE_EVERY' steps taken '''\n",
        "        self.t_step = (self.t_step + 1) % UPDATE_EVERY\n",
        "        if self.t_step == 0:\n",
        "\n",
        "            self.qnetwork_target.load_state_dict(self.qnetwork_local.state_dict())\n",
        "\n",
        "    def act(self, state, eps=0.):\n",
        "\n",
        "        state = torch.from_numpy(state).float().unsqueeze(0).to(device)\n",
        "        self.qnetwork_local.eval()\n",
        "        with torch.no_grad():\n",
        "            action_values = self.qnetwork_local(state)\n",
        "        self.qnetwork_local.train()\n",
        "\n",
        "        ''' Epsilon-greedy action selection (Already Present) '''\n",
        "        if random.random() > eps:\n",
        "            return np.argmax(action_values.cpu().data.numpy())\n",
        "        else:\n",
        "            return random.choice(np.arange(self.action_size))\n",
        "\n",
        "    def learn(self, experiences, gamma):\n",
        "        \"\"\" +E EXPERIENCE REPLAY PRESENT \"\"\"\n",
        "        states, actions, rewards, next_states, dones = experiences\n",
        "\n",
        "        ''' Get max predicted Q values (for next states) from target model'''\n",
        "        Q_targets_next = self.qnetwork_target(next_states).detach().max(1)[0].unsqueeze(1)\n",
        "\n",
        "        ''' Compute Q targets for current states '''\n",
        "        Q_targets = rewards + (gamma * Q_targets_next * (1 - dones))\n",
        "\n",
        "        ''' Get expected Q values from local model '''\n",
        "        Q_expected = self.qnetwork_local(states).gather(1, actions)\n",
        "\n",
        "        ''' Compute loss '''\n",
        "        loss = F.mse_loss(Q_expected, Q_targets)\n",
        "\n",
        "        ''' Minimize the loss '''\n",
        "        self.optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "\n",
        "        ''' Gradiant Clipping '''\n",
        "        \"\"\" +T TRUNCATION PRESENT \"\"\"\n",
        "        for param in self.qnetwork_local.parameters():\n",
        "            param.grad.data.clamp_(-1, 1)\n",
        "\n",
        "        self.optimizer.step()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 332
        },
        "id": "W8hiiFBjFFIz",
        "outputId": "4c8bee4d-f250-4b18-98ff-da64d9ed858d"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'ReplayBuffer' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-11-0540ba4240c7>\u001b[0m in \u001b[0;36m<cell line: 51>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0mbegin_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdatetime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdatetime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m \u001b[0magent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTutorialAgent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstate_shape\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0maction_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maction_shape\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mseed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m \u001b[0mrewards\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdqn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-9-7077678ea774>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, state_size, action_size, seed)\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0;34m''' Replay memory '''\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmemory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mReplayBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBUFFER_SIZE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBATCH_SIZE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0;34m''' Initialize time step (for updating every UPDATE_EVERY steps)           -Needed for Q Targets '''\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'ReplayBuffer' is not defined"
          ]
        }
      ],
      "source": [
        "''' Defining DQN Algorithm '''\n",
        "\n",
        "state_shape = env.observation_space.shape[0]\n",
        "action_shape = env.action_space.n\n",
        "\n",
        "\n",
        "def dqn(n_episodes=10000, max_t=1000, eps_start=1.0, eps_end=0.01, eps_decay=0.995):\n",
        "\n",
        "    scores_window = deque(maxlen=100)\n",
        "    ''' last 100 scores for checking if the avg is more than 195 '''\n",
        "    '''list to store rewards'''\n",
        "    rewards =[]\n",
        "\n",
        "    eps = eps_start\n",
        "    ''' initialize epsilon '''\n",
        "\n",
        "    for i_episode in range(1, n_episodes+1):\n",
        "        state = env.reset()\n",
        "        score = 0\n",
        "        for t in range(max_t):\n",
        "            action = agent.act(state, eps)\n",
        "            next_state, reward, done, _ = env.step(action)\n",
        "            agent.step(state, action, reward, next_state, done)\n",
        "            state = next_state\n",
        "            score += reward\n",
        "            if done:\n",
        "                break\n",
        "\n",
        "        scores_window.append(score)\n",
        "        rewards.append(score)\n",
        "\n",
        "        '''eps won't affect the softmax selection'''\n",
        "        '''we are taking constant tau for softmax whereas linear decay of epsilon is employed in epsilon greedy'''\n",
        "\n",
        "        eps = max(eps_end, eps_decay*eps)\n",
        "        ''' decrease epsilon '''\n",
        "\n",
        "        print('\\rEpisode {}\\tAverage Score: {:.2f}'.format(i_episode, np.mean(scores_window)), end=\"\")\n",
        "\n",
        "        if i_episode % 100 == 0:\n",
        "           print('\\rEpisode {}\\tAverage Score: {:.2f}'.format(i_episode, np.mean(scores_window)))\n",
        "        if np.mean(scores_window)>=195.0:\n",
        "           print('\\nEnvironment solved in {:d} episodes!\\tAverage Score: {:.2f}'.format(i_episode, np.mean(scores_window)))\n",
        "           break\n",
        "    return rewards\n",
        "\n",
        "''' Trial run to check if algorithm runs and saves the data '''\n",
        "\n",
        "begin_time = datetime.datetime.now()\n",
        "\n",
        "agent = TutorialAgent(state_size=state_shape,action_size = action_shape,seed = 0)\n",
        "rewards = dqn()\n",
        "\n",
        "time_taken = datetime.datetime.now() - begin_time\n",
        "\n",
        "print(time_taken)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r3yCrEbrTh7G",
        "outputId": "16085998-a1a9-4287-9e01-896b4662ab3f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Episode 100\tAverage Score: 23.47\n",
            "Episode 200\tAverage Score: 22.42\n",
            "Episode 300\tAverage Score: 24.22\n",
            "Episode 400\tAverage Score: 26.12\n",
            "Episode 500\tAverage Score: 25.37\n",
            "Episode 600\tAverage Score: 27.47\n",
            "Episode 700\tAverage Score: 29.40\n",
            "Episode 800\tAverage Score: 28.49\n",
            "Episode 900\tAverage Score: 32.54\n",
            "Episode 1000\tAverage Score: 32.50\n",
            "Episode 1100\tAverage Score: 36.19\n",
            "Episode 1200\tAverage Score: 44.04\n",
            "Episode 1300\tAverage Score: 45.18\n",
            "Episode 1400\tAverage Score: 43.45\n",
            "Episode 1500\tAverage Score: 42.07\n",
            "Episode 1600\tAverage Score: 50.26\n",
            "Episode 1700\tAverage Score: 52.52\n",
            "Episode 1800\tAverage Score: 53.87\n",
            "Episode 1900\tAverage Score: 62.72\n",
            "Episode 2000\tAverage Score: 59.66\n",
            "Episode 2100\tAverage Score: 60.15\n",
            "Episode 2200\tAverage Score: 63.79\n",
            "Episode 2300\tAverage Score: 61.84\n",
            "Episode 2400\tAverage Score: 71.07\n",
            "Episode 2500\tAverage Score: 73.48\n",
            "Episode 2600\tAverage Score: 66.21\n",
            "Episode 2700\tAverage Score: 76.29\n",
            "Episode 2800\tAverage Score: 82.82\n",
            "Episode 2900\tAverage Score: 84.93\n",
            "Episode 3000\tAverage Score: 107.93\n",
            "Episode 3100\tAverage Score: 125.43\n",
            "Episode 3200\tAverage Score: 107.25\n",
            "Episode 3300\tAverage Score: 138.07\n",
            "Episode 3400\tAverage Score: 155.62\n",
            "Episode 3500\tAverage Score: 141.24\n",
            "Episode 3600\tAverage Score: 161.09\n",
            "Episode 3700\tAverage Score: 145.43\n",
            "Episode 3800\tAverage Score: 178.50\n",
            "Episode 3900\tAverage Score: 191.93\n",
            "\n",
            "Environment solved in 3916 episodes!\tAverage Score: 197.11\n",
            "Time taken: 0:04:51.560574\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "import random\n",
        "import datetime\n",
        "import gym\n",
        "\n",
        "# Define the policy network\n",
        "class PolicyNetwork(nn.Module):\n",
        "    def __init__(self, state_size, action_size, seed, fc1_units=128, fc2_units=64):\n",
        "        super(PolicyNetwork, self).__init__()\n",
        "        self.seed = torch.manual_seed(seed)\n",
        "        self.fc1 = nn.Linear(state_size, fc1_units)\n",
        "        self.fc2 = nn.Linear(fc1_units, fc2_units)\n",
        "        self.fc3 = nn.Linear(fc2_units, action_size)\n",
        "\n",
        "    def forward(self, state):\n",
        "        x = torch.relu(self.fc1(state))\n",
        "        x = torch.relu(self.fc2(x))\n",
        "        return self.fc3(x)\n",
        "\n",
        "# Define the REINFORCE agent\n",
        "class REINFORCEAgent():\n",
        "    def __init__(self, state_size, action_size, seed, lr=0.01):\n",
        "        self.state_size = state_size\n",
        "        self.action_size = action_size\n",
        "        self.seed = random.seed(seed)\n",
        "\n",
        "        self.policy_network = PolicyNetwork(state_size, action_size, seed).to(device)\n",
        "        self.optimizer = optim.Adam(self.policy_network.parameters(), lr=lr)\n",
        "\n",
        "        self.saved_log_probs = []\n",
        "        self.rewards = []\n",
        "\n",
        "    def act(self, state):\n",
        "        state = torch.from_numpy(state).float().unsqueeze(0).to(device)\n",
        "        logits = self.policy_network(state)\n",
        "        probs = torch.softmax(logits, dim=1)\n",
        "        m = torch.distributions.Categorical(probs)\n",
        "        action = m.sample()\n",
        "        self.saved_log_probs.append(m.log_prob(action))\n",
        "        return action.item()\n",
        "\n",
        "    def learn(self, gamma):\n",
        "        discounts = [gamma ** i for i in range(len(self.rewards) + 1)]\n",
        "        R = sum([a * b for a, b in zip(discounts, self.rewards)])\n",
        "\n",
        "        policy_loss = []\n",
        "        for log_prob in self.saved_log_probs:\n",
        "            policy_loss.append(-log_prob * R)\n",
        "        policy_loss = torch.cat(policy_loss).sum()\n",
        "\n",
        "        self.optimizer.zero_grad()\n",
        "        policy_loss.backward()\n",
        "        self.optimizer.step()\n",
        "\n",
        "        self.saved_log_probs = []\n",
        "        self.rewards = []\n",
        "\n",
        "# Define the REINFORCE algorithm\n",
        "def reinforce(env, agent, n_episodes=10000, max_t=1000, gamma=0.99):\n",
        "    scores_window = deque(maxlen=100)\n",
        "    rewards = []\n",
        "    for i_episode in range(1, n_episodes+1):\n",
        "        state = env.reset()\n",
        "        score = 0\n",
        "        for t in range(max_t):\n",
        "            action = agent.act(state)\n",
        "            next_state, reward, done, _ = env.step(action)\n",
        "            agent.rewards.append(reward)\n",
        "            state = next_state\n",
        "            score += reward\n",
        "            if done:\n",
        "                break\n",
        "\n",
        "        scores_window.append(score)\n",
        "        rewards.append(score)\n",
        "\n",
        "        agent.learn(gamma)\n",
        "\n",
        "        if i_episode % 100 == 0:\n",
        "            print('\\rEpisode {}\\tAverage Score: {:.2f}'.format(i_episode, np.mean(scores_window)))\n",
        "\n",
        "        if np.mean(scores_window) >= 195.0:\n",
        "            print('\\nEnvironment solved in {:d} episodes!\\tAverage Score: {:.2f}'.format(i_episode, np.mean(scores_window)))\n",
        "            break\n",
        "\n",
        "    return rewards\n",
        "\n",
        "# Set device\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Set random seed for reproducibility\n",
        "seed = 0\n",
        "random.seed(seed)\n",
        "np.random.seed(seed)\n",
        "torch.manual_seed(seed)\n",
        "\n",
        "# Define hyperparameters\n",
        "LR = 1e-04  # Learning rate\n",
        "gamma = 0.99  # Discount factor\n",
        "n_episodes = 10000  # Maximum number of episodes\n",
        "max_t = 10000  # Maximum number of timesteps per episode\n",
        "\n",
        "# Define the environment\n",
        "# Replace 'CartPole-v1' with the name of your environment\n",
        "env = gym.make('CartPole-v1')  # Example environment (CartPole-v1)\n",
        "\n",
        "# Get state and action sizes\n",
        "state_size = env.observation_space.shape[0]\n",
        "action_size = env.action_space.n\n",
        "\n",
        "# Create an instance of the REINFORCE agent\n",
        "agent = REINFORCEAgent(state_size, action_size, seed=seed, lr=LR)\n",
        "\n",
        "# Run the REINFORCE algorithm\n",
        "begin_time = datetime.datetime.now()\n",
        "rewards = reinforce(env, agent, n_episodes=n_episodes, max_t=max_t, gamma=gamma)\n",
        "time_taken = datetime.datetime.now() - begin_time\n",
        "print(\"Time taken:\", time_taken)\n",
        "\n",
        "# Close the environment\n",
        "env.close()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "2WTu0hEvTji6",
        "outputId": "eadca11b-a8e1-458d-f588-110c954fa457"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Episode 100\tAverage Score: -496.05\n",
            "Episode 200\tAverage Score: -491.20\n",
            "Episode 300\tAverage Score: -493.67\n",
            "Episode 400\tAverage Score: -498.33\n",
            "Episode 500\tAverage Score: -495.48\n",
            "Episode 600\tAverage Score: -496.77\n",
            "Episode 700\tAverage Score: -493.16\n",
            "Episode 800\tAverage Score: -489.44\n",
            "Episode 900\tAverage Score: -494.39\n",
            "Episode 1000\tAverage Score: -486.38\n",
            "Episode 1100\tAverage Score: -475.50\n",
            "Episode 1200\tAverage Score: -460.22\n",
            "Episode 1300\tAverage Score: -438.56\n",
            "Episode 1400\tAverage Score: -469.49\n",
            "Episode 1500\tAverage Score: -456.82\n",
            "Episode 1600\tAverage Score: -454.15\n",
            "Episode 1700\tAverage Score: -464.88\n",
            "Episode 1800\tAverage Score: -477.82\n",
            "Episode 1900\tAverage Score: -480.87\n",
            "Episode 2000\tAverage Score: -495.31\n",
            "Episode 2100\tAverage Score: -462.76\n",
            "Episode 2200\tAverage Score: -479.16\n",
            "Episode 2300\tAverage Score: -487.86\n",
            "Episode 2400\tAverage Score: -470.57\n",
            "Episode 2500\tAverage Score: -480.80\n",
            "Episode 2600\tAverage Score: -476.02\n",
            "Episode 2700\tAverage Score: -487.22\n",
            "Episode 2800\tAverage Score: -457.36\n",
            "Episode 2900\tAverage Score: -436.09\n",
            "Episode 3000\tAverage Score: -399.64\n",
            "Episode 3100\tAverage Score: -389.15\n",
            "Episode 3200\tAverage Score: -390.91\n",
            "Episode 3300\tAverage Score: -397.35\n",
            "Episode 3400\tAverage Score: -386.01\n",
            "Episode 3500\tAverage Score: -373.34\n",
            "Episode 3600\tAverage Score: -309.93\n",
            "Episode 3700\tAverage Score: -290.13\n",
            "Episode 3800\tAverage Score: -269.20\n",
            "Episode 3900\tAverage Score: -277.50\n",
            "Episode 4000\tAverage Score: -261.54\n",
            "Episode 4100\tAverage Score: -289.25\n",
            "Episode 4200\tAverage Score: -291.83\n",
            "Episode 4300\tAverage Score: -308.34\n",
            "Episode 4400\tAverage Score: -308.72\n",
            "Episode 4500\tAverage Score: -326.66\n",
            "Episode 4600\tAverage Score: -335.00\n",
            "Episode 4700\tAverage Score: -338.14\n",
            "Episode 4800\tAverage Score: -359.03\n",
            "Episode 4900\tAverage Score: -374.73\n",
            "Episode 5000\tAverage Score: -382.52\n",
            "Episode 5100\tAverage Score: -372.47\n",
            "Episode 5200\tAverage Score: -349.01\n",
            "Episode 5300\tAverage Score: -318.43\n",
            "Episode 5400\tAverage Score: -332.93\n",
            "Episode 5500\tAverage Score: -304.34\n",
            "Episode 5600\tAverage Score: -314.66\n",
            "Episode 5700\tAverage Score: -320.98\n",
            "Episode 5800\tAverage Score: -321.26\n",
            "Episode 5900\tAverage Score: -294.25\n",
            "Episode 6000\tAverage Score: -259.96\n",
            "Episode 6100\tAverage Score: -256.24\n",
            "Episode 6200\tAverage Score: -256.77\n",
            "Episode 6300\tAverage Score: -268.40\n",
            "Episode 6400\tAverage Score: -266.90\n",
            "Episode 6500\tAverage Score: -288.59\n",
            "Episode 6600\tAverage Score: -266.05\n",
            "Episode 6700\tAverage Score: -286.68\n",
            "Episode 6800\tAverage Score: -295.08\n",
            "Episode 6900\tAverage Score: -283.81\n",
            "Episode 7000\tAverage Score: -283.02\n",
            "Episode 7100\tAverage Score: -322.46\n",
            "Episode 7200\tAverage Score: -316.21\n",
            "Episode 7300\tAverage Score: -323.67\n",
            "Episode 7400\tAverage Score: -330.54\n",
            "Episode 7500\tAverage Score: -369.04\n",
            "Episode 7600\tAverage Score: -346.64\n",
            "Episode 7700\tAverage Score: -323.80\n",
            "Episode 7800\tAverage Score: -325.84\n",
            "Episode 7900\tAverage Score: -323.06\n",
            "Episode 8000\tAverage Score: -304.91\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "import random\n",
        "import datetime\n",
        "import gym\n",
        "\n",
        "# Define the policy network\n",
        "class PolicyNetwork(nn.Module):\n",
        "    def __init__(self, state_size, action_size, seed, fc1_units=128, fc2_units=64):\n",
        "        super(PolicyNetwork, self).__init__()\n",
        "        self.seed = torch.manual_seed(seed)\n",
        "        self.fc1 = nn.Linear(state_size, fc1_units)\n",
        "        self.fc2 = nn.Linear(fc1_units, fc2_units)\n",
        "        self.fc3 = nn.Linear(fc2_units, action_size)\n",
        "\n",
        "    def forward(self, state):\n",
        "        x = torch.relu(self.fc1(state))\n",
        "        x = torch.relu(self.fc2(x))\n",
        "        return self.fc3(x)\n",
        "\n",
        "# Define the REINFORCE agent\n",
        "class REINFORCEAgent():\n",
        "    def __init__(self, state_size, action_size, seed, lr=0.01):\n",
        "        self.state_size = state_size\n",
        "        self.action_size = action_size\n",
        "        self.seed = random.seed(seed)\n",
        "\n",
        "        self.policy_network = PolicyNetwork(state_size, action_size, seed).to(device)\n",
        "        self.optimizer = optim.Adam(self.policy_network.parameters(), lr=lr)\n",
        "\n",
        "        self.saved_log_probs = []\n",
        "        self.rewards = []\n",
        "\n",
        "    def act(self, state):\n",
        "        state = torch.from_numpy(state).float().unsqueeze(0).to(device)\n",
        "        logits = self.policy_network(state)\n",
        "        probs = torch.softmax(logits, dim=1)\n",
        "        m = torch.distributions.Categorical(probs)\n",
        "        action = m.sample()\n",
        "        self.saved_log_probs.append(m.log_prob(action))\n",
        "        return action.item()\n",
        "\n",
        "    def learn(self, gamma):\n",
        "        discounts = [gamma ** i for i in range(len(self.rewards) + 1)]\n",
        "        R = sum([a * b for a, b in zip(discounts, self.rewards)])\n",
        "\n",
        "        policy_loss = []\n",
        "        for log_prob in self.saved_log_probs:\n",
        "            policy_loss.append(-log_prob * R)\n",
        "        policy_loss = torch.cat(policy_loss).sum()\n",
        "\n",
        "        self.optimizer.zero_grad()\n",
        "        policy_loss.backward()\n",
        "        self.optimizer.step()\n",
        "\n",
        "        self.saved_log_probs = []\n",
        "        self.rewards = []\n",
        "\n",
        "# Define the REINFORCE algorithm\n",
        "def reinforce(env, agent, n_episodes=10000, max_t=1000, gamma=0.99):\n",
        "    scores_window = deque(maxlen=100)\n",
        "    rewards = []\n",
        "    for i_episode in range(1, n_episodes+1):\n",
        "        state = env.reset()\n",
        "        score = 0\n",
        "        for t in range(max_t):\n",
        "            action = agent.act(state)\n",
        "            next_state, reward, done, _ = env.step(action)\n",
        "            agent.rewards.append(reward)\n",
        "            state = next_state\n",
        "            score += reward\n",
        "            if done:\n",
        "                break\n",
        "\n",
        "        scores_window.append(score)\n",
        "        rewards.append(score)\n",
        "\n",
        "        agent.learn(gamma)\n",
        "\n",
        "        if i_episode % 100 == 0:\n",
        "            print('\\rEpisode {}\\tAverage Score: {:.2f}'.format(i_episode, np.mean(scores_window)))\n",
        "\n",
        "        if np.mean(scores_window) >= 195.0:\n",
        "            print('\\nEnvironment solved in {:d} episodes!\\tAverage Score: {:.2f}'.format(i_episode, np.mean(scores_window)))\n",
        "            break\n",
        "\n",
        "    return rewards\n",
        "\n",
        "# Set device\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Set random seed for reproducibility\n",
        "seed = 0\n",
        "random.seed(seed)\n",
        "np.random.seed(seed)\n",
        "torch.manual_seed(seed)\n",
        "\n",
        "# Define hyperparameters\n",
        "LR = 1e-04  # Learning rate\n",
        "gamma = 0.99  # Discount factor\n",
        "n_episodes = 10000  # Maximum number of episodes\n",
        "max_t = 10000  # Maximum number of timesteps per episode\n",
        "\n",
        "# Define the environment\n",
        "# Replace 'CartPole-v1' with the name of your environment\n",
        "env = gym.make('Acrobot-v1')  # Example environment (CartPole-v1)\n",
        "\n",
        "# Get state and action sizes\n",
        "state_size = env.observation_space.shape[0]\n",
        "action_size = env.action_space.n\n",
        "\n",
        "# Create an instance of the REINFORCE agent\n",
        "agent = REINFORCEAgent(state_size, action_size, seed=seed, lr=LR)\n",
        "\n",
        "# Run the REINFORCE algorithm\n",
        "begin_time = datetime.datetime.now()\n",
        "rewards = reinforce(env, agent, n_episodes=n_episodes, max_t=max_t, gamma=gamma)\n",
        "time_taken = datetime.datetime.now() - begin_time\n",
        "print(\"Time taken:\", time_taken)\n",
        "\n",
        "# Close the environment\n",
        "env.close()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hU3upyEwNPDR"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}